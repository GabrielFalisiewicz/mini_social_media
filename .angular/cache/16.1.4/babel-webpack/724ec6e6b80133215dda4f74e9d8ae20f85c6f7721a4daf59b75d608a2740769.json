{"ast":null,"code":"'use strict';\n\nvar _asyncToGenerator = require(\"C:/Users/GabrielFalisiewiczS/OneDrive/Dokumenty/Udemy/backend/kurs1/node_modules/@babel/runtime/helpers/asyncToGenerator.js\").default;\nconst crypto = require('crypto');\nconst {\n  appendFile,\n  mkdir,\n  readFile,\n  readdir,\n  rm,\n  writeFile\n} = require('fs/promises');\nconst {\n  Minipass\n} = require('minipass');\nconst path = require('path');\nconst ssri = require('ssri');\nconst uniqueFilename = require('unique-filename');\nconst contentPath = require('./content/path');\nconst hashToSegments = require('./util/hash-to-segments');\nconst indexV = require('../package.json')['cache-version'].index;\nconst {\n  moveFile\n} = require('@npmcli/fs');\nmodule.exports.NotFoundError = class NotFoundError extends Error {\n  constructor(cache, key) {\n    super(`No cache entry for ${key} found in ${cache}`);\n    this.code = 'ENOENT';\n    this.cache = cache;\n    this.key = key;\n  }\n};\nmodule.exports.compact = compact;\nfunction compact(_x, _x2, _x3) {\n  return _compact.apply(this, arguments);\n}\nfunction _compact() {\n  _compact = _asyncToGenerator(function* (cache, key, matchFn, opts = {}) {\n    const bucket = bucketPath(cache, key);\n    const entries = yield bucketEntries(bucket);\n    const newEntries = [];\n    // we loop backwards because the bottom-most result is the newest\n    // since we add new entries with appendFile\n    for (let i = entries.length - 1; i >= 0; --i) {\n      const entry = entries[i];\n      // a null integrity could mean either a delete was appended\n      // or the user has simply stored an index that does not map\n      // to any content. we determine if the user wants to keep the\n      // null integrity based on the validateEntry function passed in options.\n      // if the integrity is null and no validateEntry is provided, we break\n      // as we consider the null integrity to be a deletion of everything\n      // that came before it.\n      if (entry.integrity === null && !opts.validateEntry) {\n        break;\n      }\n\n      // if this entry is valid, and it is either the first entry or\n      // the newEntries array doesn't already include an entry that\n      // matches this one based on the provided matchFn, then we add\n      // it to the beginning of our list\n      if ((!opts.validateEntry || opts.validateEntry(entry) === true) && (newEntries.length === 0 || !newEntries.find(oldEntry => matchFn(oldEntry, entry)))) {\n        newEntries.unshift(entry);\n      }\n    }\n    const newIndex = '\\n' + newEntries.map(entry => {\n      const stringified = JSON.stringify(entry);\n      const hash = hashEntry(stringified);\n      return `${hash}\\t${stringified}`;\n    }).join('\\n');\n    const setup = /*#__PURE__*/function () {\n      var _ref5 = _asyncToGenerator(function* () {\n        const target = uniqueFilename(path.join(cache, 'tmp'), opts.tmpPrefix);\n        yield mkdir(path.dirname(target), {\n          recursive: true\n        });\n        return {\n          target,\n          moved: false\n        };\n      });\n      return function setup() {\n        return _ref5.apply(this, arguments);\n      };\n    }();\n    const teardown = /*#__PURE__*/function () {\n      var _ref6 = _asyncToGenerator(function* (tmp) {\n        if (!tmp.moved) {\n          return rm(tmp.target, {\n            recursive: true,\n            force: true\n          });\n        }\n      });\n      return function teardown(_x15) {\n        return _ref6.apply(this, arguments);\n      };\n    }();\n    const write = /*#__PURE__*/function () {\n      var _ref7 = _asyncToGenerator(function* (tmp) {\n        yield writeFile(tmp.target, newIndex, {\n          flag: 'wx'\n        });\n        yield mkdir(path.dirname(bucket), {\n          recursive: true\n        });\n        // we use @npmcli/move-file directly here because we\n        // want to overwrite the existing file\n        yield moveFile(tmp.target, bucket);\n        tmp.moved = true;\n      });\n      return function write(_x16) {\n        return _ref7.apply(this, arguments);\n      };\n    }();\n\n    // write the file atomically\n    const tmp = yield setup();\n    try {\n      yield write(tmp);\n    } finally {\n      yield teardown(tmp);\n    }\n\n    // we reverse the list we generated such that the newest\n    // entries come first in order to make looping through them easier\n    // the true passed to formatEntry tells it to keep null\n    // integrity values, if they made it this far it's because\n    // validateEntry returned true, and as such we should return it\n    return newEntries.reverse().map(entry => formatEntry(cache, entry, true));\n  });\n  return _compact.apply(this, arguments);\n}\nmodule.exports.insert = insert;\nfunction insert(_x4, _x5, _x6) {\n  return _insert.apply(this, arguments);\n}\nfunction _insert() {\n  _insert = _asyncToGenerator(function* (cache, key, integrity, opts = {}) {\n    const {\n      metadata,\n      size,\n      time\n    } = opts;\n    const bucket = bucketPath(cache, key);\n    const entry = {\n      key,\n      integrity: integrity && ssri.stringify(integrity),\n      time: time || Date.now(),\n      size,\n      metadata\n    };\n    try {\n      yield mkdir(path.dirname(bucket), {\n        recursive: true\n      });\n      const stringified = JSON.stringify(entry);\n      // NOTE - Cleverness ahoy!\n      //\n      // This works because it's tremendously unlikely for an entry to corrupt\n      // another while still preserving the string length of the JSON in\n      // question. So, we just slap the length in there and verify it on read.\n      //\n      // Thanks to @isaacs for the whiteboarding session that ended up with\n      // this.\n      yield appendFile(bucket, `\\n${hashEntry(stringified)}\\t${stringified}`);\n    } catch (err) {\n      if (err.code === 'ENOENT') {\n        return undefined;\n      }\n      throw err;\n    }\n    return formatEntry(cache, entry);\n  });\n  return _insert.apply(this, arguments);\n}\nmodule.exports.find = find;\nfunction find(_x7, _x8) {\n  return _find.apply(this, arguments);\n}\nfunction _find() {\n  _find = _asyncToGenerator(function* (cache, key) {\n    const bucket = bucketPath(cache, key);\n    try {\n      const entries = yield bucketEntries(bucket);\n      return entries.reduce((latest, next) => {\n        if (next && next.key === key) {\n          return formatEntry(cache, next);\n        } else {\n          return latest;\n        }\n      }, null);\n    } catch (err) {\n      if (err.code === 'ENOENT') {\n        return null;\n      } else {\n        throw err;\n      }\n    }\n  });\n  return _find.apply(this, arguments);\n}\nmodule.exports.delete = del;\nfunction del(cache, key, opts = {}) {\n  if (!opts.removeFully) {\n    return insert(cache, key, null, opts);\n  }\n  const bucket = bucketPath(cache, key);\n  return rm(bucket, {\n    recursive: true,\n    force: true\n  });\n}\nmodule.exports.lsStream = lsStream;\nfunction lsStream(cache) {\n  const indexDir = bucketDir(cache);\n  const stream = new Minipass({\n    objectMode: true\n  });\n\n  // Set all this up to run on the stream and then just return the stream\n  Promise.resolve().then( /*#__PURE__*/_asyncToGenerator(function* () {\n    const buckets = yield readdirOrEmpty(indexDir);\n    yield Promise.all(buckets.map( /*#__PURE__*/function () {\n      var _ref2 = _asyncToGenerator(function* (bucket) {\n        const bucketPath = path.join(indexDir, bucket);\n        const subbuckets = yield readdirOrEmpty(bucketPath);\n        yield Promise.all(subbuckets.map( /*#__PURE__*/function () {\n          var _ref3 = _asyncToGenerator(function* (subbucket) {\n            const subbucketPath = path.join(bucketPath, subbucket);\n\n            // \"/cachename/<bucket 0xFF>/<bucket 0xFF>./*\"\n            const subbucketEntries = yield readdirOrEmpty(subbucketPath);\n            yield Promise.all(subbucketEntries.map( /*#__PURE__*/function () {\n              var _ref4 = _asyncToGenerator(function* (entry) {\n                const entryPath = path.join(subbucketPath, entry);\n                try {\n                  const entries = yield bucketEntries(entryPath);\n                  // using a Map here prevents duplicate keys from showing up\n                  // twice, I guess?\n                  const reduced = entries.reduce((acc, entry) => {\n                    acc.set(entry.key, entry);\n                    return acc;\n                  }, new Map());\n                  // reduced is a map of key => entry\n                  for (const entry of reduced.values()) {\n                    const formatted = formatEntry(cache, entry);\n                    if (formatted) {\n                      stream.write(formatted);\n                    }\n                  }\n                } catch (err) {\n                  if (err.code === 'ENOENT') {\n                    return undefined;\n                  }\n                  throw err;\n                }\n              });\n              return function (_x11) {\n                return _ref4.apply(this, arguments);\n              };\n            }()));\n          });\n          return function (_x10) {\n            return _ref3.apply(this, arguments);\n          };\n        }()));\n      });\n      return function (_x9) {\n        return _ref2.apply(this, arguments);\n      };\n    }()));\n    stream.end();\n    return stream;\n  })).catch(err => stream.emit('error', err));\n  return stream;\n}\nmodule.exports.ls = ls;\nfunction ls(_x12) {\n  return _ls.apply(this, arguments);\n}\nfunction _ls() {\n  _ls = _asyncToGenerator(function* (cache) {\n    const entries = yield lsStream(cache).collect();\n    return entries.reduce((acc, xs) => {\n      acc[xs.key] = xs;\n      return acc;\n    }, {});\n  });\n  return _ls.apply(this, arguments);\n}\nmodule.exports.bucketEntries = bucketEntries;\nfunction bucketEntries(_x13, _x14) {\n  return _bucketEntries2.apply(this, arguments);\n}\nfunction _bucketEntries2() {\n  _bucketEntries2 = _asyncToGenerator(function* (bucket, filter) {\n    const data = yield readFile(bucket, 'utf8');\n    return _bucketEntries(data, filter);\n  });\n  return _bucketEntries2.apply(this, arguments);\n}\nfunction _bucketEntries(data, filter) {\n  const entries = [];\n  data.split('\\n').forEach(entry => {\n    if (!entry) {\n      return;\n    }\n    const pieces = entry.split('\\t');\n    if (!pieces[1] || hashEntry(pieces[1]) !== pieces[0]) {\n      // Hash is no good! Corruption or malice? Doesn't matter!\n      // EJECT EJECT\n      return;\n    }\n    let obj;\n    try {\n      obj = JSON.parse(pieces[1]);\n    } catch (_) {\n      // eslint-ignore-next-line no-empty-block\n    }\n    // coverage disabled here, no need to test with an entry that parses to something falsey\n    // istanbul ignore else\n    if (obj) {\n      entries.push(obj);\n    }\n  });\n  return entries;\n}\nmodule.exports.bucketDir = bucketDir;\nfunction bucketDir(cache) {\n  return path.join(cache, `index-v${indexV}`);\n}\nmodule.exports.bucketPath = bucketPath;\nfunction bucketPath(cache, key) {\n  const hashed = hashKey(key);\n  return path.join.apply(path, [bucketDir(cache)].concat(hashToSegments(hashed)));\n}\nmodule.exports.hashKey = hashKey;\nfunction hashKey(key) {\n  return hash(key, 'sha256');\n}\nmodule.exports.hashEntry = hashEntry;\nfunction hashEntry(str) {\n  return hash(str, 'sha1');\n}\nfunction hash(str, digest) {\n  return crypto.createHash(digest).update(str).digest('hex');\n}\nfunction formatEntry(cache, entry, keepAll) {\n  // Treat null digests as deletions. They'll shadow any previous entries.\n  if (!entry.integrity && !keepAll) {\n    return null;\n  }\n  return {\n    key: entry.key,\n    integrity: entry.integrity,\n    path: entry.integrity ? contentPath(cache, entry.integrity) : undefined,\n    size: entry.size,\n    time: entry.time,\n    metadata: entry.metadata\n  };\n}\nfunction readdirOrEmpty(dir) {\n  return readdir(dir).catch(err => {\n    if (err.code === 'ENOENT' || err.code === 'ENOTDIR') {\n      return [];\n    }\n    throw err;\n  });\n}","map":{"version":3,"names":["_asyncToGenerator","require","default","crypto","appendFile","mkdir","readFile","readdir","rm","writeFile","Minipass","path","ssri","uniqueFilename","contentPath","hashToSegments","indexV","index","moveFile","module","exports","NotFoundError","Error","constructor","cache","key","code","compact","_x","_x2","_x3","_compact","apply","arguments","matchFn","opts","bucket","bucketPath","entries","bucketEntries","newEntries","i","length","entry","integrity","validateEntry","find","oldEntry","unshift","newIndex","map","stringified","JSON","stringify","hash","hashEntry","join","setup","_ref5","target","tmpPrefix","dirname","recursive","moved","teardown","_ref6","tmp","force","_x15","write","_ref7","flag","_x16","reverse","formatEntry","insert","_x4","_x5","_x6","_insert","metadata","size","time","Date","now","err","undefined","_x7","_x8","_find","reduce","latest","next","delete","del","removeFully","lsStream","indexDir","bucketDir","stream","objectMode","Promise","resolve","then","buckets","readdirOrEmpty","all","_ref2","subbuckets","_ref3","subbucket","subbucketPath","subbucketEntries","_ref4","entryPath","reduced","acc","set","Map","values","formatted","_x11","_x10","_x9","end","catch","emit","ls","_x12","_ls","collect","xs","_x13","_x14","_bucketEntries2","filter","data","_bucketEntries","split","forEach","pieces","obj","parse","_","push","hashed","hashKey","concat","str","digest","createHash","update","keepAll","dir"],"sources":["C:/Users/GabrielFalisiewiczS/OneDrive/Dokumenty/Udemy/backend/kurs1/node_modules/cacache/lib/entry-index.js"],"sourcesContent":["'use strict'\n\nconst crypto = require('crypto')\nconst {\n  appendFile,\n  mkdir,\n  readFile,\n  readdir,\n  rm,\n  writeFile,\n} = require('fs/promises')\nconst { Minipass } = require('minipass')\nconst path = require('path')\nconst ssri = require('ssri')\nconst uniqueFilename = require('unique-filename')\n\nconst contentPath = require('./content/path')\nconst hashToSegments = require('./util/hash-to-segments')\nconst indexV = require('../package.json')['cache-version'].index\nconst { moveFile } = require('@npmcli/fs')\n\nmodule.exports.NotFoundError = class NotFoundError extends Error {\n  constructor (cache, key) {\n    super(`No cache entry for ${key} found in ${cache}`)\n    this.code = 'ENOENT'\n    this.cache = cache\n    this.key = key\n  }\n}\n\nmodule.exports.compact = compact\n\nasync function compact (cache, key, matchFn, opts = {}) {\n  const bucket = bucketPath(cache, key)\n  const entries = await bucketEntries(bucket)\n  const newEntries = []\n  // we loop backwards because the bottom-most result is the newest\n  // since we add new entries with appendFile\n  for (let i = entries.length - 1; i >= 0; --i) {\n    const entry = entries[i]\n    // a null integrity could mean either a delete was appended\n    // or the user has simply stored an index that does not map\n    // to any content. we determine if the user wants to keep the\n    // null integrity based on the validateEntry function passed in options.\n    // if the integrity is null and no validateEntry is provided, we break\n    // as we consider the null integrity to be a deletion of everything\n    // that came before it.\n    if (entry.integrity === null && !opts.validateEntry) {\n      break\n    }\n\n    // if this entry is valid, and it is either the first entry or\n    // the newEntries array doesn't already include an entry that\n    // matches this one based on the provided matchFn, then we add\n    // it to the beginning of our list\n    if ((!opts.validateEntry || opts.validateEntry(entry) === true) &&\n      (newEntries.length === 0 ||\n        !newEntries.find((oldEntry) => matchFn(oldEntry, entry)))) {\n      newEntries.unshift(entry)\n    }\n  }\n\n  const newIndex = '\\n' + newEntries.map((entry) => {\n    const stringified = JSON.stringify(entry)\n    const hash = hashEntry(stringified)\n    return `${hash}\\t${stringified}`\n  }).join('\\n')\n\n  const setup = async () => {\n    const target = uniqueFilename(path.join(cache, 'tmp'), opts.tmpPrefix)\n    await mkdir(path.dirname(target), { recursive: true })\n    return {\n      target,\n      moved: false,\n    }\n  }\n\n  const teardown = async (tmp) => {\n    if (!tmp.moved) {\n      return rm(tmp.target, { recursive: true, force: true })\n    }\n  }\n\n  const write = async (tmp) => {\n    await writeFile(tmp.target, newIndex, { flag: 'wx' })\n    await mkdir(path.dirname(bucket), { recursive: true })\n    // we use @npmcli/move-file directly here because we\n    // want to overwrite the existing file\n    await moveFile(tmp.target, bucket)\n    tmp.moved = true\n  }\n\n  // write the file atomically\n  const tmp = await setup()\n  try {\n    await write(tmp)\n  } finally {\n    await teardown(tmp)\n  }\n\n  // we reverse the list we generated such that the newest\n  // entries come first in order to make looping through them easier\n  // the true passed to formatEntry tells it to keep null\n  // integrity values, if they made it this far it's because\n  // validateEntry returned true, and as such we should return it\n  return newEntries.reverse().map((entry) => formatEntry(cache, entry, true))\n}\n\nmodule.exports.insert = insert\n\nasync function insert (cache, key, integrity, opts = {}) {\n  const { metadata, size, time } = opts\n  const bucket = bucketPath(cache, key)\n  const entry = {\n    key,\n    integrity: integrity && ssri.stringify(integrity),\n    time: time || Date.now(),\n    size,\n    metadata,\n  }\n  try {\n    await mkdir(path.dirname(bucket), { recursive: true })\n    const stringified = JSON.stringify(entry)\n    // NOTE - Cleverness ahoy!\n    //\n    // This works because it's tremendously unlikely for an entry to corrupt\n    // another while still preserving the string length of the JSON in\n    // question. So, we just slap the length in there and verify it on read.\n    //\n    // Thanks to @isaacs for the whiteboarding session that ended up with\n    // this.\n    await appendFile(bucket, `\\n${hashEntry(stringified)}\\t${stringified}`)\n  } catch (err) {\n    if (err.code === 'ENOENT') {\n      return undefined\n    }\n\n    throw err\n  }\n  return formatEntry(cache, entry)\n}\n\nmodule.exports.find = find\n\nasync function find (cache, key) {\n  const bucket = bucketPath(cache, key)\n  try {\n    const entries = await bucketEntries(bucket)\n    return entries.reduce((latest, next) => {\n      if (next && next.key === key) {\n        return formatEntry(cache, next)\n      } else {\n        return latest\n      }\n    }, null)\n  } catch (err) {\n    if (err.code === 'ENOENT') {\n      return null\n    } else {\n      throw err\n    }\n  }\n}\n\nmodule.exports.delete = del\n\nfunction del (cache, key, opts = {}) {\n  if (!opts.removeFully) {\n    return insert(cache, key, null, opts)\n  }\n\n  const bucket = bucketPath(cache, key)\n  return rm(bucket, { recursive: true, force: true })\n}\n\nmodule.exports.lsStream = lsStream\n\nfunction lsStream (cache) {\n  const indexDir = bucketDir(cache)\n  const stream = new Minipass({ objectMode: true })\n\n  // Set all this up to run on the stream and then just return the stream\n  Promise.resolve().then(async () => {\n    const buckets = await readdirOrEmpty(indexDir)\n    await Promise.all(buckets.map(async (bucket) => {\n      const bucketPath = path.join(indexDir, bucket)\n      const subbuckets = await readdirOrEmpty(bucketPath)\n      await Promise.all(subbuckets.map(async (subbucket) => {\n        const subbucketPath = path.join(bucketPath, subbucket)\n\n        // \"/cachename/<bucket 0xFF>/<bucket 0xFF>./*\"\n        const subbucketEntries = await readdirOrEmpty(subbucketPath)\n        await Promise.all(subbucketEntries.map(async (entry) => {\n          const entryPath = path.join(subbucketPath, entry)\n          try {\n            const entries = await bucketEntries(entryPath)\n            // using a Map here prevents duplicate keys from showing up\n            // twice, I guess?\n            const reduced = entries.reduce((acc, entry) => {\n              acc.set(entry.key, entry)\n              return acc\n            }, new Map())\n            // reduced is a map of key => entry\n            for (const entry of reduced.values()) {\n              const formatted = formatEntry(cache, entry)\n              if (formatted) {\n                stream.write(formatted)\n              }\n            }\n          } catch (err) {\n            if (err.code === 'ENOENT') {\n              return undefined\n            }\n            throw err\n          }\n        }))\n      }))\n    }))\n    stream.end()\n    return stream\n  }).catch(err => stream.emit('error', err))\n\n  return stream\n}\n\nmodule.exports.ls = ls\n\nasync function ls (cache) {\n  const entries = await lsStream(cache).collect()\n  return entries.reduce((acc, xs) => {\n    acc[xs.key] = xs\n    return acc\n  }, {})\n}\n\nmodule.exports.bucketEntries = bucketEntries\n\nasync function bucketEntries (bucket, filter) {\n  const data = await readFile(bucket, 'utf8')\n  return _bucketEntries(data, filter)\n}\n\nfunction _bucketEntries (data, filter) {\n  const entries = []\n  data.split('\\n').forEach((entry) => {\n    if (!entry) {\n      return\n    }\n\n    const pieces = entry.split('\\t')\n    if (!pieces[1] || hashEntry(pieces[1]) !== pieces[0]) {\n      // Hash is no good! Corruption or malice? Doesn't matter!\n      // EJECT EJECT\n      return\n    }\n    let obj\n    try {\n      obj = JSON.parse(pieces[1])\n    } catch (_) {\n      // eslint-ignore-next-line no-empty-block\n    }\n    // coverage disabled here, no need to test with an entry that parses to something falsey\n    // istanbul ignore else\n    if (obj) {\n      entries.push(obj)\n    }\n  })\n  return entries\n}\n\nmodule.exports.bucketDir = bucketDir\n\nfunction bucketDir (cache) {\n  return path.join(cache, `index-v${indexV}`)\n}\n\nmodule.exports.bucketPath = bucketPath\n\nfunction bucketPath (cache, key) {\n  const hashed = hashKey(key)\n  return path.join.apply(\n    path,\n    [bucketDir(cache)].concat(hashToSegments(hashed))\n  )\n}\n\nmodule.exports.hashKey = hashKey\n\nfunction hashKey (key) {\n  return hash(key, 'sha256')\n}\n\nmodule.exports.hashEntry = hashEntry\n\nfunction hashEntry (str) {\n  return hash(str, 'sha1')\n}\n\nfunction hash (str, digest) {\n  return crypto\n    .createHash(digest)\n    .update(str)\n    .digest('hex')\n}\n\nfunction formatEntry (cache, entry, keepAll) {\n  // Treat null digests as deletions. They'll shadow any previous entries.\n  if (!entry.integrity && !keepAll) {\n    return null\n  }\n\n  return {\n    key: entry.key,\n    integrity: entry.integrity,\n    path: entry.integrity ? contentPath(cache, entry.integrity) : undefined,\n    size: entry.size,\n    time: entry.time,\n    metadata: entry.metadata,\n  }\n}\n\nfunction readdirOrEmpty (dir) {\n  return readdir(dir).catch((err) => {\n    if (err.code === 'ENOENT' || err.code === 'ENOTDIR') {\n      return []\n    }\n\n    throw err\n  })\n}\n"],"mappings":"AAAA,YAAY;;AAAA,IAAAA,iBAAA,GAAAC,OAAA,gIAAAC,OAAA;AAEZ,MAAMC,MAAM,GAAGF,OAAO,CAAC,QAAQ,CAAC;AAChC,MAAM;EACJG,UAAU;EACVC,KAAK;EACLC,QAAQ;EACRC,OAAO;EACPC,EAAE;EACFC;AACF,CAAC,GAAGR,OAAO,CAAC,aAAa,CAAC;AAC1B,MAAM;EAAES;AAAS,CAAC,GAAGT,OAAO,CAAC,UAAU,CAAC;AACxC,MAAMU,IAAI,GAAGV,OAAO,CAAC,MAAM,CAAC;AAC5B,MAAMW,IAAI,GAAGX,OAAO,CAAC,MAAM,CAAC;AAC5B,MAAMY,cAAc,GAAGZ,OAAO,CAAC,iBAAiB,CAAC;AAEjD,MAAMa,WAAW,GAAGb,OAAO,CAAC,gBAAgB,CAAC;AAC7C,MAAMc,cAAc,GAAGd,OAAO,CAAC,yBAAyB,CAAC;AACzD,MAAMe,MAAM,GAAGf,OAAO,CAAC,iBAAiB,CAAC,CAAC,eAAe,CAAC,CAACgB,KAAK;AAChE,MAAM;EAAEC;AAAS,CAAC,GAAGjB,OAAO,CAAC,YAAY,CAAC;AAE1CkB,MAAM,CAACC,OAAO,CAACC,aAAa,GAAG,MAAMA,aAAa,SAASC,KAAK,CAAC;EAC/DC,WAAWA,CAAEC,KAAK,EAAEC,GAAG,EAAE;IACvB,KAAK,CAAE,sBAAqBA,GAAI,aAAYD,KAAM,EAAC,CAAC;IACpD,IAAI,CAACE,IAAI,GAAG,QAAQ;IACpB,IAAI,CAACF,KAAK,GAAGA,KAAK;IAClB,IAAI,CAACC,GAAG,GAAGA,GAAG;EAChB;AACF,CAAC;AAEDN,MAAM,CAACC,OAAO,CAACO,OAAO,GAAGA,OAAO;AAAA,SAEjBA,OAAOA,CAAAC,EAAA,EAAAC,GAAA,EAAAC,GAAA;EAAA,OAAAC,QAAA,CAAAC,KAAA,OAAAC,SAAA;AAAA;AAAA,SAAAF,SAAA;EAAAA,QAAA,GAAA/B,iBAAA,CAAtB,WAAwBwB,KAAK,EAAEC,GAAG,EAAES,OAAO,EAAEC,IAAI,GAAG,CAAC,CAAC,EAAE;IACtD,MAAMC,MAAM,GAAGC,UAAU,CAACb,KAAK,EAAEC,GAAG,CAAC;IACrC,MAAMa,OAAO,SAASC,aAAa,CAACH,MAAM,CAAC;IAC3C,MAAMI,UAAU,GAAG,EAAE;IACrB;IACA;IACA,KAAK,IAAIC,CAAC,GAAGH,OAAO,CAACI,MAAM,GAAG,CAAC,EAAED,CAAC,IAAI,CAAC,EAAE,EAAEA,CAAC,EAAE;MAC5C,MAAME,KAAK,GAAGL,OAAO,CAACG,CAAC,CAAC;MACxB;MACA;MACA;MACA;MACA;MACA;MACA;MACA,IAAIE,KAAK,CAACC,SAAS,KAAK,IAAI,IAAI,CAACT,IAAI,CAACU,aAAa,EAAE;QACnD;MACF;;MAEA;MACA;MACA;MACA;MACA,IAAI,CAAC,CAACV,IAAI,CAACU,aAAa,IAAIV,IAAI,CAACU,aAAa,CAACF,KAAK,CAAC,KAAK,IAAI,MAC3DH,UAAU,CAACE,MAAM,KAAK,CAAC,IACtB,CAACF,UAAU,CAACM,IAAI,CAAEC,QAAQ,IAAKb,OAAO,CAACa,QAAQ,EAAEJ,KAAK,CAAC,CAAC,CAAC,EAAE;QAC7DH,UAAU,CAACQ,OAAO,CAACL,KAAK,CAAC;MAC3B;IACF;IAEA,MAAMM,QAAQ,GAAG,IAAI,GAAGT,UAAU,CAACU,GAAG,CAAEP,KAAK,IAAK;MAChD,MAAMQ,WAAW,GAAGC,IAAI,CAACC,SAAS,CAACV,KAAK,CAAC;MACzC,MAAMW,IAAI,GAAGC,SAAS,CAACJ,WAAW,CAAC;MACnC,OAAQ,GAAEG,IAAK,KAAIH,WAAY,EAAC;IAClC,CAAC,CAAC,CAACK,IAAI,CAAC,IAAI,CAAC;IAEb,MAAMC,KAAK;MAAA,IAAAC,KAAA,GAAA1D,iBAAA,CAAG,aAAY;QACxB,MAAM2D,MAAM,GAAG9C,cAAc,CAACF,IAAI,CAAC6C,IAAI,CAAChC,KAAK,EAAE,KAAK,CAAC,EAAEW,IAAI,CAACyB,SAAS,CAAC;QACtE,MAAMvD,KAAK,CAACM,IAAI,CAACkD,OAAO,CAACF,MAAM,CAAC,EAAE;UAAEG,SAAS,EAAE;QAAK,CAAC,CAAC;QACtD,OAAO;UACLH,MAAM;UACNI,KAAK,EAAE;QACT,CAAC;MACH,CAAC;MAAA,gBAPKN,KAAKA,CAAA;QAAA,OAAAC,KAAA,CAAA1B,KAAA,OAAAC,SAAA;MAAA;IAAA,GAOV;IAED,MAAM+B,QAAQ;MAAA,IAAAC,KAAA,GAAAjE,iBAAA,CAAG,WAAOkE,GAAG,EAAK;QAC9B,IAAI,CAACA,GAAG,CAACH,KAAK,EAAE;UACd,OAAOvD,EAAE,CAAC0D,GAAG,CAACP,MAAM,EAAE;YAAEG,SAAS,EAAE,IAAI;YAAEK,KAAK,EAAE;UAAK,CAAC,CAAC;QACzD;MACF,CAAC;MAAA,gBAJKH,QAAQA,CAAAI,IAAA;QAAA,OAAAH,KAAA,CAAAjC,KAAA,OAAAC,SAAA;MAAA;IAAA,GAIb;IAED,MAAMoC,KAAK;MAAA,IAAAC,KAAA,GAAAtE,iBAAA,CAAG,WAAOkE,GAAG,EAAK;QAC3B,MAAMzD,SAAS,CAACyD,GAAG,CAACP,MAAM,EAAEV,QAAQ,EAAE;UAAEsB,IAAI,EAAE;QAAK,CAAC,CAAC;QACrD,MAAMlE,KAAK,CAACM,IAAI,CAACkD,OAAO,CAACzB,MAAM,CAAC,EAAE;UAAE0B,SAAS,EAAE;QAAK,CAAC,CAAC;QACtD;QACA;QACA,MAAM5C,QAAQ,CAACgD,GAAG,CAACP,MAAM,EAAEvB,MAAM,CAAC;QAClC8B,GAAG,CAACH,KAAK,GAAG,IAAI;MAClB,CAAC;MAAA,gBAPKM,KAAKA,CAAAG,IAAA;QAAA,OAAAF,KAAA,CAAAtC,KAAA,OAAAC,SAAA;MAAA;IAAA,GAOV;;IAED;IACA,MAAMiC,GAAG,SAAST,KAAK,CAAC,CAAC;IACzB,IAAI;MACF,MAAMY,KAAK,CAACH,GAAG,CAAC;IAClB,CAAC,SAAS;MACR,MAAMF,QAAQ,CAACE,GAAG,CAAC;IACrB;;IAEA;IACA;IACA;IACA;IACA;IACA,OAAO1B,UAAU,CAACiC,OAAO,CAAC,CAAC,CAACvB,GAAG,CAAEP,KAAK,IAAK+B,WAAW,CAAClD,KAAK,EAAEmB,KAAK,EAAE,IAAI,CAAC,CAAC;EAC7E,CAAC;EAAA,OAAAZ,QAAA,CAAAC,KAAA,OAAAC,SAAA;AAAA;AAEDd,MAAM,CAACC,OAAO,CAACuD,MAAM,GAAGA,MAAM;AAAA,SAEfA,MAAMA,CAAAC,GAAA,EAAAC,GAAA,EAAAC,GAAA;EAAA,OAAAC,OAAA,CAAA/C,KAAA,OAAAC,SAAA;AAAA;AAAA,SAAA8C,QAAA;EAAAA,OAAA,GAAA/E,iBAAA,CAArB,WAAuBwB,KAAK,EAAEC,GAAG,EAAEmB,SAAS,EAAET,IAAI,GAAG,CAAC,CAAC,EAAE;IACvD,MAAM;MAAE6C,QAAQ;MAAEC,IAAI;MAAEC;IAAK,CAAC,GAAG/C,IAAI;IACrC,MAAMC,MAAM,GAAGC,UAAU,CAACb,KAAK,EAAEC,GAAG,CAAC;IACrC,MAAMkB,KAAK,GAAG;MACZlB,GAAG;MACHmB,SAAS,EAAEA,SAAS,IAAIhC,IAAI,CAACyC,SAAS,CAACT,SAAS,CAAC;MACjDsC,IAAI,EAAEA,IAAI,IAAIC,IAAI,CAACC,GAAG,CAAC,CAAC;MACxBH,IAAI;MACJD;IACF,CAAC;IACD,IAAI;MACF,MAAM3E,KAAK,CAACM,IAAI,CAACkD,OAAO,CAACzB,MAAM,CAAC,EAAE;QAAE0B,SAAS,EAAE;MAAK,CAAC,CAAC;MACtD,MAAMX,WAAW,GAAGC,IAAI,CAACC,SAAS,CAACV,KAAK,CAAC;MACzC;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA,MAAMvC,UAAU,CAACgC,MAAM,EAAG,KAAImB,SAAS,CAACJ,WAAW,CAAE,KAAIA,WAAY,EAAC,CAAC;IACzE,CAAC,CAAC,OAAOkC,GAAG,EAAE;MACZ,IAAIA,GAAG,CAAC3D,IAAI,KAAK,QAAQ,EAAE;QACzB,OAAO4D,SAAS;MAClB;MAEA,MAAMD,GAAG;IACX;IACA,OAAOX,WAAW,CAAClD,KAAK,EAAEmB,KAAK,CAAC;EAClC,CAAC;EAAA,OAAAoC,OAAA,CAAA/C,KAAA,OAAAC,SAAA;AAAA;AAEDd,MAAM,CAACC,OAAO,CAAC0B,IAAI,GAAGA,IAAI;AAAA,SAEXA,IAAIA,CAAAyC,GAAA,EAAAC,GAAA;EAAA,OAAAC,KAAA,CAAAzD,KAAA,OAAAC,SAAA;AAAA;AAAA,SAAAwD,MAAA;EAAAA,KAAA,GAAAzF,iBAAA,CAAnB,WAAqBwB,KAAK,EAAEC,GAAG,EAAE;IAC/B,MAAMW,MAAM,GAAGC,UAAU,CAACb,KAAK,EAAEC,GAAG,CAAC;IACrC,IAAI;MACF,MAAMa,OAAO,SAASC,aAAa,CAACH,MAAM,CAAC;MAC3C,OAAOE,OAAO,CAACoD,MAAM,CAAC,CAACC,MAAM,EAAEC,IAAI,KAAK;QACtC,IAAIA,IAAI,IAAIA,IAAI,CAACnE,GAAG,KAAKA,GAAG,EAAE;UAC5B,OAAOiD,WAAW,CAAClD,KAAK,EAAEoE,IAAI,CAAC;QACjC,CAAC,MAAM;UACL,OAAOD,MAAM;QACf;MACF,CAAC,EAAE,IAAI,CAAC;IACV,CAAC,CAAC,OAAON,GAAG,EAAE;MACZ,IAAIA,GAAG,CAAC3D,IAAI,KAAK,QAAQ,EAAE;QACzB,OAAO,IAAI;MACb,CAAC,MAAM;QACL,MAAM2D,GAAG;MACX;IACF;EACF,CAAC;EAAA,OAAAI,KAAA,CAAAzD,KAAA,OAAAC,SAAA;AAAA;AAEDd,MAAM,CAACC,OAAO,CAACyE,MAAM,GAAGC,GAAG;AAE3B,SAASA,GAAGA,CAAEtE,KAAK,EAAEC,GAAG,EAAEU,IAAI,GAAG,CAAC,CAAC,EAAE;EACnC,IAAI,CAACA,IAAI,CAAC4D,WAAW,EAAE;IACrB,OAAOpB,MAAM,CAACnD,KAAK,EAAEC,GAAG,EAAE,IAAI,EAAEU,IAAI,CAAC;EACvC;EAEA,MAAMC,MAAM,GAAGC,UAAU,CAACb,KAAK,EAAEC,GAAG,CAAC;EACrC,OAAOjB,EAAE,CAAC4B,MAAM,EAAE;IAAE0B,SAAS,EAAE,IAAI;IAAEK,KAAK,EAAE;EAAK,CAAC,CAAC;AACrD;AAEAhD,MAAM,CAACC,OAAO,CAAC4E,QAAQ,GAAGA,QAAQ;AAElC,SAASA,QAAQA,CAAExE,KAAK,EAAE;EACxB,MAAMyE,QAAQ,GAAGC,SAAS,CAAC1E,KAAK,CAAC;EACjC,MAAM2E,MAAM,GAAG,IAAIzF,QAAQ,CAAC;IAAE0F,UAAU,EAAE;EAAK,CAAC,CAAC;;EAEjD;EACAC,OAAO,CAACC,OAAO,CAAC,CAAC,CAACC,IAAI,eAAAvG,iBAAA,CAAC,aAAY;IACjC,MAAMwG,OAAO,SAASC,cAAc,CAACR,QAAQ,CAAC;IAC9C,MAAMI,OAAO,CAACK,GAAG,CAACF,OAAO,CAACtD,GAAG;MAAA,IAAAyD,KAAA,GAAA3G,iBAAA,CAAC,WAAOoC,MAAM,EAAK;QAC9C,MAAMC,UAAU,GAAG1B,IAAI,CAAC6C,IAAI,CAACyC,QAAQ,EAAE7D,MAAM,CAAC;QAC9C,MAAMwE,UAAU,SAASH,cAAc,CAACpE,UAAU,CAAC;QACnD,MAAMgE,OAAO,CAACK,GAAG,CAACE,UAAU,CAAC1D,GAAG;UAAA,IAAA2D,KAAA,GAAA7G,iBAAA,CAAC,WAAO8G,SAAS,EAAK;YACpD,MAAMC,aAAa,GAAGpG,IAAI,CAAC6C,IAAI,CAACnB,UAAU,EAAEyE,SAAS,CAAC;;YAEtD;YACA,MAAME,gBAAgB,SAASP,cAAc,CAACM,aAAa,CAAC;YAC5D,MAAMV,OAAO,CAACK,GAAG,CAACM,gBAAgB,CAAC9D,GAAG;cAAA,IAAA+D,KAAA,GAAAjH,iBAAA,CAAC,WAAO2C,KAAK,EAAK;gBACtD,MAAMuE,SAAS,GAAGvG,IAAI,CAAC6C,IAAI,CAACuD,aAAa,EAAEpE,KAAK,CAAC;gBACjD,IAAI;kBACF,MAAML,OAAO,SAASC,aAAa,CAAC2E,SAAS,CAAC;kBAC9C;kBACA;kBACA,MAAMC,OAAO,GAAG7E,OAAO,CAACoD,MAAM,CAAC,CAAC0B,GAAG,EAAEzE,KAAK,KAAK;oBAC7CyE,GAAG,CAACC,GAAG,CAAC1E,KAAK,CAAClB,GAAG,EAAEkB,KAAK,CAAC;oBACzB,OAAOyE,GAAG;kBACZ,CAAC,EAAE,IAAIE,GAAG,CAAC,CAAC,CAAC;kBACb;kBACA,KAAK,MAAM3E,KAAK,IAAIwE,OAAO,CAACI,MAAM,CAAC,CAAC,EAAE;oBACpC,MAAMC,SAAS,GAAG9C,WAAW,CAAClD,KAAK,EAAEmB,KAAK,CAAC;oBAC3C,IAAI6E,SAAS,EAAE;sBACbrB,MAAM,CAAC9B,KAAK,CAACmD,SAAS,CAAC;oBACzB;kBACF;gBACF,CAAC,CAAC,OAAOnC,GAAG,EAAE;kBACZ,IAAIA,GAAG,CAAC3D,IAAI,KAAK,QAAQ,EAAE;oBACzB,OAAO4D,SAAS;kBAClB;kBACA,MAAMD,GAAG;gBACX;cACF,CAAC;cAAA,iBAAAoC,IAAA;gBAAA,OAAAR,KAAA,CAAAjF,KAAA,OAAAC,SAAA;cAAA;YAAA,IAAC,CAAC;UACL,CAAC;UAAA,iBAAAyF,IAAA;YAAA,OAAAb,KAAA,CAAA7E,KAAA,OAAAC,SAAA;UAAA;QAAA,IAAC,CAAC;MACL,CAAC;MAAA,iBAAA0F,GAAA;QAAA,OAAAhB,KAAA,CAAA3E,KAAA,OAAAC,SAAA;MAAA;IAAA,IAAC,CAAC;IACHkE,MAAM,CAACyB,GAAG,CAAC,CAAC;IACZ,OAAOzB,MAAM;EACf,CAAC,EAAC,CAAC0B,KAAK,CAACxC,GAAG,IAAIc,MAAM,CAAC2B,IAAI,CAAC,OAAO,EAAEzC,GAAG,CAAC,CAAC;EAE1C,OAAOc,MAAM;AACf;AAEAhF,MAAM,CAACC,OAAO,CAAC2G,EAAE,GAAGA,EAAE;AAAA,SAEPA,EAAEA,CAAAC,IAAA;EAAA,OAAAC,GAAA,CAAAjG,KAAA,OAAAC,SAAA;AAAA;AAAA,SAAAgG,IAAA;EAAAA,GAAA,GAAAjI,iBAAA,CAAjB,WAAmBwB,KAAK,EAAE;IACxB,MAAMc,OAAO,SAAS0D,QAAQ,CAACxE,KAAK,CAAC,CAAC0G,OAAO,CAAC,CAAC;IAC/C,OAAO5F,OAAO,CAACoD,MAAM,CAAC,CAAC0B,GAAG,EAAEe,EAAE,KAAK;MACjCf,GAAG,CAACe,EAAE,CAAC1G,GAAG,CAAC,GAAG0G,EAAE;MAChB,OAAOf,GAAG;IACZ,CAAC,EAAE,CAAC,CAAC,CAAC;EACR,CAAC;EAAA,OAAAa,GAAA,CAAAjG,KAAA,OAAAC,SAAA;AAAA;AAEDd,MAAM,CAACC,OAAO,CAACmB,aAAa,GAAGA,aAAa;AAAA,SAE7BA,aAAaA,CAAA6F,IAAA,EAAAC,IAAA;EAAA,OAAAC,eAAA,CAAAtG,KAAA,OAAAC,SAAA;AAAA;AAAA,SAAAqG,gBAAA;EAAAA,eAAA,GAAAtI,iBAAA,CAA5B,WAA8BoC,MAAM,EAAEmG,MAAM,EAAE;IAC5C,MAAMC,IAAI,SAASlI,QAAQ,CAAC8B,MAAM,EAAE,MAAM,CAAC;IAC3C,OAAOqG,cAAc,CAACD,IAAI,EAAED,MAAM,CAAC;EACrC,CAAC;EAAA,OAAAD,eAAA,CAAAtG,KAAA,OAAAC,SAAA;AAAA;AAED,SAASwG,cAAcA,CAAED,IAAI,EAAED,MAAM,EAAE;EACrC,MAAMjG,OAAO,GAAG,EAAE;EAClBkG,IAAI,CAACE,KAAK,CAAC,IAAI,CAAC,CAACC,OAAO,CAAEhG,KAAK,IAAK;IAClC,IAAI,CAACA,KAAK,EAAE;MACV;IACF;IAEA,MAAMiG,MAAM,GAAGjG,KAAK,CAAC+F,KAAK,CAAC,IAAI,CAAC;IAChC,IAAI,CAACE,MAAM,CAAC,CAAC,CAAC,IAAIrF,SAAS,CAACqF,MAAM,CAAC,CAAC,CAAC,CAAC,KAAKA,MAAM,CAAC,CAAC,CAAC,EAAE;MACpD;MACA;MACA;IACF;IACA,IAAIC,GAAG;IACP,IAAI;MACFA,GAAG,GAAGzF,IAAI,CAAC0F,KAAK,CAACF,MAAM,CAAC,CAAC,CAAC,CAAC;IAC7B,CAAC,CAAC,OAAOG,CAAC,EAAE;MACV;IAAA;IAEF;IACA;IACA,IAAIF,GAAG,EAAE;MACPvG,OAAO,CAAC0G,IAAI,CAACH,GAAG,CAAC;IACnB;EACF,CAAC,CAAC;EACF,OAAOvG,OAAO;AAChB;AAEAnB,MAAM,CAACC,OAAO,CAAC8E,SAAS,GAAGA,SAAS;AAEpC,SAASA,SAASA,CAAE1E,KAAK,EAAE;EACzB,OAAOb,IAAI,CAAC6C,IAAI,CAAChC,KAAK,EAAG,UAASR,MAAO,EAAC,CAAC;AAC7C;AAEAG,MAAM,CAACC,OAAO,CAACiB,UAAU,GAAGA,UAAU;AAEtC,SAASA,UAAUA,CAAEb,KAAK,EAAEC,GAAG,EAAE;EAC/B,MAAMwH,MAAM,GAAGC,OAAO,CAACzH,GAAG,CAAC;EAC3B,OAAOd,IAAI,CAAC6C,IAAI,CAACxB,KAAK,CACpBrB,IAAI,EACJ,CAACuF,SAAS,CAAC1E,KAAK,CAAC,CAAC,CAAC2H,MAAM,CAACpI,cAAc,CAACkI,MAAM,CAAC,CAClD,CAAC;AACH;AAEA9H,MAAM,CAACC,OAAO,CAAC8H,OAAO,GAAGA,OAAO;AAEhC,SAASA,OAAOA,CAAEzH,GAAG,EAAE;EACrB,OAAO6B,IAAI,CAAC7B,GAAG,EAAE,QAAQ,CAAC;AAC5B;AAEAN,MAAM,CAACC,OAAO,CAACmC,SAAS,GAAGA,SAAS;AAEpC,SAASA,SAASA,CAAE6F,GAAG,EAAE;EACvB,OAAO9F,IAAI,CAAC8F,GAAG,EAAE,MAAM,CAAC;AAC1B;AAEA,SAAS9F,IAAIA,CAAE8F,GAAG,EAAEC,MAAM,EAAE;EAC1B,OAAOlJ,MAAM,CACVmJ,UAAU,CAACD,MAAM,CAAC,CAClBE,MAAM,CAACH,GAAG,CAAC,CACXC,MAAM,CAAC,KAAK,CAAC;AAClB;AAEA,SAAS3E,WAAWA,CAAElD,KAAK,EAAEmB,KAAK,EAAE6G,OAAO,EAAE;EAC3C;EACA,IAAI,CAAC7G,KAAK,CAACC,SAAS,IAAI,CAAC4G,OAAO,EAAE;IAChC,OAAO,IAAI;EACb;EAEA,OAAO;IACL/H,GAAG,EAAEkB,KAAK,CAAClB,GAAG;IACdmB,SAAS,EAAED,KAAK,CAACC,SAAS;IAC1BjC,IAAI,EAAEgC,KAAK,CAACC,SAAS,GAAG9B,WAAW,CAACU,KAAK,EAAEmB,KAAK,CAACC,SAAS,CAAC,GAAG0C,SAAS;IACvEL,IAAI,EAAEtC,KAAK,CAACsC,IAAI;IAChBC,IAAI,EAAEvC,KAAK,CAACuC,IAAI;IAChBF,QAAQ,EAAErC,KAAK,CAACqC;EAClB,CAAC;AACH;AAEA,SAASyB,cAAcA,CAAEgD,GAAG,EAAE;EAC5B,OAAOlJ,OAAO,CAACkJ,GAAG,CAAC,CAAC5B,KAAK,CAAExC,GAAG,IAAK;IACjC,IAAIA,GAAG,CAAC3D,IAAI,KAAK,QAAQ,IAAI2D,GAAG,CAAC3D,IAAI,KAAK,SAAS,EAAE;MACnD,OAAO,EAAE;IACX;IAEA,MAAM2D,GAAG;EACX,CAAC,CAAC;AACJ"},"metadata":{},"sourceType":"script","externalDependencies":[]}